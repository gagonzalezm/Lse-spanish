% !TEX root = ../pdf/lsj.tex
% [There are multiple lsj.tex files, but the one in ../pdf is the usual one]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Why do we learn statistics?~\label{ch:whystats}}


\begin{verse}{\it
``Thou shalt not answer questionnaires \\
Or quizzes upon World Affairs, \\
\hspace*{.5cm}    Nor with compliance \\
Take any test. Thou shalt not sit  \\
With statisticians nor commit \\
\hspace*{.5cm}    A social science" }\vspace*{6pt} \\ 
\hspace*{2cm} -- W.H. Auden\FOOTNOTE{The quote comes from Auden's 1946 poem {\it Under Which Lyre: A Reactionary Tract for the Times}, delivered as part of a commencement address at Harvard University. The history of the poem is kind of interesting: \url{http://harvardmagazine.com/2007/11/a-poets-warning.html}}
\end{verse}
\vspace*{12pt}


\section{On the psychology of statistics~\label{sec:whywhywhy}}

Para sorpresa de muchos estudiantes, la estadística es una parte importante de 
los programas de estudio de Psicología. Para sorpresa de nadie, la estadística 
es muy raramente la parte favorita de estudiar psicología. Después de todo, si 
te gusta o amas la estadística, muy probablemente te hubieras matriculado para 
estudiar estadística en vez de psicología. Entonces, no es sorprendente, que una
gran proporción de estudiantes no estén felices con el hecho de que la psicología
tiene mucho de estadística.  Desde este punto de vista, pienso que el lugar 
correcto para empezar a responder algunas de las preguntas más comunes que las 
personas hacen sobre estadística. 
 
Gran parte de este tema va de la mano con la idea misma de estadística. ¿Qué es?
¿Para qué está? ¿Y por qué los científicos parecen estar tan increíblemente 
obsesionados con ésta?  Todas esas son preguntas buenas cuando lo piensas. 
Vamos a empezar con la última. Como grupo, los científicos parecen estar 
empecinados extrañamente en hacer análisis estadísticos a todo. De hecho, 
hacemos análisis estadísticos tan a menudo que olvidamos explicarle a la gente 
porque lo hacemos. Esto es un tipo de mandato de fe entre los científicos – y 
especialmente entre científicos sociales- que tus hallazgos no pueden ser
creíbles hasta que tu hayas hecho algo de estadísticas. Estudiantes de pregrado
podrían ser perdonados por pensar que estamos totalmente locos, porque nadie se
toma el tiempo para responder una simple pregunta: 
\begin{quote}
{\it ¿Por qué haces estadística? ¿Por qué los científicos no usan simplemente el /underline{sentido común}?}
\end{quote}

Es una pregunta ingenua en cierta forma, pero la mayoría de las preguntas buenas lo son. Hay un montón de respuestas buenas para esta, pero por mi dinero, pero la mejor respuesta es simplemente una:  No confiamos lo suficiente en nosotros mismos. Nos preocupamos que somos humanos y susceptibles a todos los sesgos, tentaciones y debilidades que los humanos sufren. Mucho de la estadística es, básicamente, una protección. Usar “Sentido común” para evaluar evidencia significa confiar en nuestros instintos viscerales, apoyar en argumentos verbales y usar el poder bruto de la razón humana para llegar a la respuesta correcta. La mayoría de los científicos no piensan que este enfoque funcione probablemente.

De hecho, llegar pensar en esto, para mí suena un montón como una pregunta psicología y desde que trabajo en un departamento de psicología, parece buena idea cavar un poco más profundo aquí. ¿Es realmente plausible que en enfoque desde el “sentido común” es muy confiable? Argumentos verbales son construidos en un lenguaje, y todos los lenguajes tienen sesgos -- algunas cosas son más difíciles de decir que otras, y no necesariamente por eso son falsas (ej. La electrodinámica cuántica es una buena teoría, pero difícil de explicar en palabras). Nuestros instintos “viscerales” no son diseñados para resolver problemas científicos. Estos están diseñados para lidiar con inferencias del día a día -- Y dado que la evolución biológica es más lenta que el cambio cultural, podríamos decir que estos instintos están diseñados para resolver problemas del día a día de un {\it mundo diferente} del que ahora habitamos. Más fundamentalmente, razonar concienzudamente requiere que las personas se involucren en una ``inducción'', haciendo suposiciones sabias y yendo más allá de la evidencia inmediata de los sentidos para hacer generalizaciones sobre el mundo. Si tú crees que puedes hacer eso sin ser influenciado por varios distractores, bien, yo tengo un puente en Londres y me gustaría vendértelo. Diablos, como la siguiente sección muestra, no podemos siquiera resolver problemas deductivos (unos donde las suposiciones no son requeridas, sin ser influenciados por nuestros sesgos pre-existentes

\SUBSECTION{La maldición del sesgo de la creencia}

Las personas son en su mayoría muy inteligentes. Somos ciertamente más inteligentes que la otras especies con las que compartimos este planeta ( pese a que algunos podrían estar en desacuerdo). Nuestras mentes son cosas tan sorprendentes y parecemos capaces de las más increíbles proezas de pensamiento y razón. Aunque eso no nos hace perfectos. Y entre muchas cosas que los psicólogos han mostrado con los años, es que es realmente difícil para nosotros ser neutrales, evaluar evidencia imparcialmente, y sin estar inclinados por sesgos pre-existentes. Un ejemplo de esto es el efecto del sesgo de creencia en razonamiento lógico: si tú le pides a una persona evaluar si un razonamiento es lógicamente valido(esto es, la conclusión debería ser correcta si las premisas son verdaderas), nosotros tendemos a estar influenciados por la credibilidad de la conclusión, incluso cuando no deberíamos. Por ejemplo, acá hay un argumento valido donde la conclusión es creíble: 
\begin{quote}
Todos los cigarrillos son costosos (Premisa 1) \\
Algunas cosas adictivas no son costosas (Premisa 2)\\
Por lo tanto, algunas cosas adictivas no son cigarrillos(Conclusión)
\end{quote}
Y aquí un argumento válido donde la conclusión no es creíble:
\begin{quote}
Todas las cosas adictivas son costosas  (Premisa 1))\\
Algunos cigarrillos no son costosos (Premisa 2)\\
Por lo tanto, agunos cigarillos no son adictivos
\end{quote}
La   {\it estructura} lógica del argumento \#2 es idéntica a la estructura del argumento \#1, y ellas son ambas válidas. Sin embargo, en el segundo argumento, hay buenas razones para pensar que la premisa 1 es incorrecta, y como resultado es probable que la conclusión también es incorrecta.  Pero eso es totalmente irrelevante al topico en mano; un argumento es deduciblemente valido si la conclusión es una consecuencia lógica de las premisas. Esto es, un argumento válido no tiene que involucrar declaraciones verdaderas.

En la otra mano, aquí hay un argumento inválido que tiene una conclusión creíble
\begin{quote}
Todas las cosas adictivas son costosas (Premisa 1)\\
Algunos cigarrillos no son costosos (Premisa 2)\\
Por lo tanto, algunas cosas adictivas no son cigarrillos (Conclusión)
\end{quote}
Y finalmente, un argumento invalido con una conclusion increíble:
\begin{quote}
Todos los cigarrillos son costosos (Premisa 1)\\
Algunas cosas adictivas no son costosas (Premisa 2) (Premise 2)\\
Por lo tanto, algunos cigarrillos no son adictivos (Conclusion)
\end{quote}
Now, suppose that people really are perfectly able to set aside their pre-existing biases about what is true and what isn't, and purely evaluate an argument on its logical merits. We'd expect 100\% of people to say that the valid arguments are valid, and 0\% of people to say that the invalid arguments are valid. So if you ran an experiment looking at this, you'd expect to see data like this:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l|cc|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{conclusion feels true} & \multicolumn{1}{c}{conclusion feels false} \\ \cline{2-3}
argument is valid   & 100\% say ``valid'' & 100\%  say ``valid''\\ 
argument is invalid &  0\% say ``valid''& 0\% say ``valid''\\ \cline{2-3}
\end{tabular}
\end{center}

\noindent
If the psychological data looked like this (or even a good approximation to this), we might feel safe in just trusting our gut instincts. That is, it'd be perfectly okay just to let scientists evaluate data based on their common sense, and not bother with all this murky statistics stuff. However, you guys have taken psych classes, and by now you probably know where this is going.

In a classic study, \textcite{Evans1983} ran an experiment looking at exactly this. What they found is that when pre-existing biases (i.e., beliefs) were in agreement with the structure of the data, everything went the way you'd hope: 
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l|cc|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{conclusion feels true} & \multicolumn{1}{c}{conclusion feels false} \\ \cline{2-3}
argument is valid   & 92\% say ``valid''&  \\ 
argument is invalid &  & 8\% say ``valid''\\ \cline{2-3}
\end{tabular}
\end{center}
Not perfect, but that's pretty good. But look what happens when our intuitive feelings about the truth of the conclusion run against the logical structure of the argument:
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l|cc|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{conclusion feels true} & \multicolumn{1}{c}{conclusion feels false} \\ \cline{2-3}
argument is valid   & 92\% say ``valid'' & {\bf 46\% say ``valid''} \\
argument is invalid & {\bf 92\% say ``valid''} & 8\% say ``valid'' \\ \cline{2-3}
\end{tabular}
\end{center}
Oh dear, that's not as good. Apparently, when people are presented with a strong argument that contradicts our pre-existing beliefs, we find it pretty hard to even perceive it to be a strong argument (people only did so 46\% of the time). Even worse, when people are presented with a weak argument that agrees with our pre-existing biases, almost no-one can see that the argument is weak (people got that one wrong 92\% of the time!).\FOOTNOTE{In my more cynical moments I feel like this fact alone explains 95\% of what I read on the internet.}

If you think about it, it's not as if these data are horribly damning. Overall, people did do better than chance at compensating for their prior biases, since about 60\% of people's judgements were correct (you'd expect 50\% by chance). Even so, if you were a professional ``evaluator of evidence'', and someone came along and offered you a magic tool that improves your chances of making the right decision from 60\% to (say) 95\%, you'd probably jump at it, right? Of course you would. Thankfully, we actually do have a tool that can do this. But it's not magic, it's statistics. So that's reason \#1 why scientists love statistics. It's just {\it too easy} for us to ``believe what we want to believe''. So instead, if we want to ``believe in the data'', we're going to need a bit of help to keep our personal biases under control. That's what statistics does,  it helps keep us honest.



\section{The cautionary tale of Simpson's paradox}

%Clifford H. Wagner (February 1982). "Simpson's Paradox in Real Life". The American Statistician 36 (1): 46-48.

%Sex Bias in Graduate Admissions: Data from Berkeley
%P. J. Bickel, E. A. Hammel and J. W. O'Connell
%Science
%New Series, Vol. 187, No. 4175 (Feb. 7, 1975), pp. 398-404

The following is a true story (I think!). In 1973, the University of California, Berkeley had some worries about the admissions of students into their postgraduate courses. Specifically, the thing that caused the problem was that the gender breakdown of their admissions, which looked like this:
\begin{center}
\begin{tabular}{lcc}
& Number of applicants & Percent admitted \\
Males & 8442 & 44\% \\
Females & 4321 & 35\%  \\
\end{tabular}
\end{center}
Given this, they were worried about being sued!\footnote{Earlier versions of these notes incorrectly suggested that they actually were sued. But that's not true. There's a nice commentary on this here: \url{https://www.refsmmat.com/posts/2016-05-08-simpsons-paradox-berkeley.html}. A big thank you to Wilfried Van Hirtum for pointing this out to me.} Given that there were nearly 13,000 applicants, a difference of 9\% in admission rates between males and females is just way too big to be a coincidence. Pretty compelling data, right? And if I were to say to you that these data {\it actually} reflect a weak bias in favour of women (sort of!), you'd probably think that I was either crazy or sexist. 

Oddly, it's actually sort of true. When people started looking more carefully at the admissions data they told a rather different story \parencite{Bickel1975}. Specifically, when they looked at it on a department by department basis, it turned out that most of the departments actually had a slightly {\it higher} success rate for female applicants than for male applicants. The table below shows the admission figures for the six largest departments (with the names of the departments removed for privacy reasons):
\begin{center}
\begin{tabular}{c|cc|cc}
	& \multicolumn{2}{c|}{Males} & \multicolumn{2}{c}{Females} \\ 
	Department & Applicants	& Percent admitted &	Applicants & Percent admitted \\ \hline
A &	825	&62\%&	108&	82\% \\
B&	560	&63\%&	25	&68\% \\
C&	325	&37\%&	593	&34\%\\
D&	417	&33\%&	375	&35\%\\
E&	191	&28\%&	393	&24\%\\
F&	272	&6\%	&	341	&7\%\\
\end{tabular}
\end{center}
\noindent
Remarkably, most departments had a {\it higher} rate of admissions for females than for males! Yet the overall rate of admission across the university for females was {\it lower} than for males.  How can this be? How can both of these statements be true at the same time?

Here's what's going on. Firstly, notice that the departments are {\it not} equal to one another in terms of their admission percentages: some departments (e.g., A, B) tended to admit a high percentage of the qualified applicants, whereas others (e.g., F) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get {\bf A}$>${\bf B}$>$D$>$C$>$F$>$E (the ``easy'' departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C$>$E$>$D$>$F$>${\bf A}$>${\bf B}. In other words, what these data seem to be suggesting is that the female applicants tended to apply to ``harder'' departments. And in fact, if we look at Figure~\ref{fig:berkeley} we see that this trend is systematic, and quite striking. This effect is known as \keyterm{Simpson's paradox}. It's not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it's real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point: doing research is hard, and there are {\it lots} of subtle, counter-intuitive traps lying in wait for the unwary.  That's reason \#2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.


\begin{figure}[t!]
\begin{center}
\epsfig{file = ../img/whystats/berkeleyadmissions3.eps, clip=true,width = 11cm}
\caption[The Berkeley 1973 college admissions data.]{The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from \textcite{Bickel1975}. Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot departments with fewer than 40 applicants. } 
\label{fig:berkeley}
\HR
\end{center}
\end{figure}




Before leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves {\it part} of the problem. Remember that we started all this with the concern that Berkeley's admissions processes might be unfairly biased against female applicants. When we looked at the ``aggregated'' data, it did seem like the university was discriminating against women, but when we ``disaggregate'' and looked at the individual behaviour of all the departments, it turned out that the actual departments were, if anything, slightly biased in favour of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments.  From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department, and there are good reasons to do that. At the level of individual departments the decisions are more or less unbiased (the weak bias in favour of females at that level is small, and not consistent across departments). Since the university can't dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce. 

That was the basis for my somewhat glib remarks earlier, but that's not exactly the whole story, is it? After all, if we're interested in this from a more sociological and psychological perspective, we might want to ask {\it why} there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to ``hard sciences'' and females prefer ``humanities''. And suppose further that the reason for why the humanities departments have low admission rates is  because the government doesn't want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are ``useless chick stuff''. That seems pretty {\it blatantly} gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you're interested in the overall structural effects of subtle gender biases, then you probably want to look at {\it both} the aggregated and disaggregated data. If you're interested in the decision making process at Berkeley itself then you're probably only interested in the disaggregated data. 

In short there are a lot of critical questions that you can't answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a {\it tool} to help you learn about your data. No more and no less. It's a powerful tool to that end, but there's no substitute for careful thought.


\section{Statistics in psychology}

I hope that the discussion above helped explain why science in general is so focused on statistics. But I'm guessing that you have a lot more questions about what role statistics plays in psychology, and specifically why psychology classes always devote so many lectures to stats. So here's my attempt to answer a few of them...

\begin{itemize}

\item {\bf Why does psychology have so much statistics?}

To be perfectly honest, there's a few different reasons, some of which are better than others. The most important reason is that psychology is a statistical science. What I mean by that is that the ``things'' that we study are {\it people}. Real, complicated, gloriously messy, infuriatingly perverse people. The ``things'' of physics include objects like electrons, and while there are all sorts of complexities that arise in physics, electrons don't have minds of their own. They don't have opinions, they don't differ from each other in weird and arbitrary ways, they don't get bored in the middle of an experiment, and they don't get angry at the experimenter and then deliberately try to sabotage the data set (not that I've ever done that!). At a fundamental level psychology is harder than physics.\FOOTNOTE{Which might explain why physics is just a teensy bit further advanced as a science than we are.}

Basically, we teach statistics to you as psychologists because you need to be better at stats than physicists. There's actually a saying used sometimes in physics, to the effect that ``if your experiment needs statistics, you should have done a better experiment". They have the luxury of being able to say that because their objects of study are pathetically simple in comparison to the vast mess that confronts social scientists. And it's not just psychology. Most social sciences are desperately reliant on statistics. Not because we're bad experimenters, but because we've picked a harder problem to solve. We teach you stats because you really, really need it.

\item {\bf Can't someone else do the statistics?}

To some extent, but not completely. It's true that you don't need to become a fully trained statistician just to do psychology, but you do need to reach a certain level of statistical competence. In my view, there's three reasons that every psychological researcher ought to be able to do basic statistics:

\begin{itemize}
\item Firstly, there's the fundamental reason: statistics is deeply intertwined with research design. If you want to be good at designing psychological studies, you need to at the very least understand the basics of stats. 
\item Secondly, if you want to be good at the psychological side of the research, then you need to be able to understand the psychological literature, right? But almost every paper in the psychological literature reports the results of statistical analyses. So if you really want to understand the psychology, you need to be able to understand what other people did with their data. And that means understanding a certain amount of statistics.
\item Thirdly, there's a big practical problem with being dependent on other people to do all your statistics: statistical analysis is {\it expensive}. If you ever get bored and want to look up how much the Australian government charges for university fees, you'll notice something interesting: statistics is designated as a ``national priority'' category, and so the fees are much, much lower than for any other area of study. This is because there's a massive shortage of statisticians out there. So, from your perspective as a psychological researcher, the laws of supply and demand aren't exactly on your side here! As a result, in almost any real life situation where you want to do psychological research, the cruel facts will be that you don't have enough money to afford a statistician. So the economics of the situation mean that you have to be pretty self-sufficient. 
\end{itemize}

\noindent
Note that a lot of these reasons generalise beyond researchers. If you want to be a practicing psychologist and stay on top of the field, it helps to be able to read the scientific literature, which relies pretty heavily on statistics. 

\item {\bf I don't care about jobs, research, or clinical work. Do I need statistics?}

Okay, now you're just messing with me. Still, I think it should matter to you too. Statistics should matter to you in the same way that statistics should matter to {\it everyone}. We live in the 21st century, and data are {\it everywhere}. Frankly, given the world in which we live these days, a basic knowledge of statistics is pretty damn close to a survival tool! Which is the topic of the next section.

\end{itemize}

\section{Statistics in everyday life}

\begin{quote}
{\it ``We are drowning in information,\\ but we are starved for knowledge''} \\ \hspace*{2cm} -- Various authors, original probably John Naisbitt
\end{quote}

\noindent
When I started writing up my lecture notes I took the 20 most recent news articles posted to the ABC news website. Of those 20 articles, it turned out that 8 of them involved a discussion of something that I would call a statistical topic and 6 of those made a mistake. The most common error, if you're curious, was failing to report baseline data (e.g., the article mentions that 5\% of people in situation X have some characteristic Y, but doesn't say how common the characteristic is for everyone else!). The point I'm trying to make here isn't that journalists are bad at statistics (though they almost always are), it's that a  basic knowledge of statistics is very helpful for trying to figure out when someone else is either making a mistake or even lying to you. In fact, one of the biggest things that a knowledge of statistics does to you is cause you to get angry at the newspaper or the internet on a far more frequent basis. You can find a good example of this in Section~\ref{sec:housingpriceexample}. In later versions of this book I'll try to include more anecdotes along those lines. 


\section{There's more to research methods than statistics}

So far, most of what I've talked about is statistics, and so you'd be forgiven for thinking that statistics is all I care about in life. To be fair, you wouldn't be far wrong, but research methodology is a broader concept than statistics. So most research methods  courses will cover a lot of topics that relate much more to the pragmatics of research design, and in particular the issues that you encounter when trying to do research with humans. However, about 99\% of student {\it fears} relate to the statistics part of the course, so I've focused on the stats in this discussion, and hopefully I've convinced you that statistics matters, and more importantly, that it's not to be feared.  That being said, it's pretty typical for introductory research methods classes to be very stats-heavy. This is not (usually) because the lecturers are evil people. Quite the contrary, in fact. Introductory classes focus a lot on the statistics because you almost always find yourself needing statistics before you need the other research methods training. Why? Because almost all of your assignments in other classes will rely on statistical training, to a much greater extent than they rely on other methodological tools. It's not common for undergraduate  assignments to require you to design your own study from the ground up (in which case you would need to know a lot about research design), but it {\it is} common for assignments to ask you to analyse and interpret data that were collected in a study that someone else designed (in which case you need statistics). In that sense, from the perspective of allowing you to do well in all your other classes, the statistics is more urgent. 

But note that ``urgent'' is different from ``important'' -- they both matter. I really do want to stress that research design is just as important as data analysis, and this book does spend a fair amount of time on it. However, while statistics has a kind of universality, and provides a set of core tools that are useful for most types of psychological research, the research methods side isn't quite so universal. There are some general principles that everyone should think about, but a lot of research design is very idiosyncratic, and is specific to the area of research that you want to engage in. To the extent that it's the details that matter, those details don't usually show up in an introductory stats and research methods class.


%\section{Plagiarism alert!}

%There's a lot of things I should explain about how I've organised this subject. Before I go into details, though, I want to acknowledge my sources. These notes came together from many years of reading, but there's some books in particular that I've relied upon. Some of them you might want to read yourself, others I strongly warn against. In order to spare you the tedium of seeing me cite the same few books over and over again, I'll just acknowledge them this once. The vast majority of the content in these notes is shamelessly lifted from the following books. On the off chance that any of these people ever read these notes, please take it as a compliment that I stole your stuff to teach to my students!

%\bigskip
%\begin{itemize}
%\item David Salsburg (2001). The lady tasting tea: How statistics revolutionized science in the twentieth century. \vspace*{6pt}

%{\it This is a lovely, easily readable discussion of the great debates in 20th century statistics. The W.H. Auden quote at the start of these notes is taken from the beginning of Salsburg's book. I thoroughly recommend reading the chapters on, Fisher and Neyman. As you'll see in this course, most of the bizarre pathologies in the statistical ``orthodoxies'' that we teach come from the fact that scientists have badly misunderstood the nature of their ideas.} \vspace*{6pt}

%\item Stephen Stigler (1986). The history of statistics: the measurement of uncertainty before 1900.  \vspace*{6pt}

%{\it This book is heavy going: unlike Salsburg's book, it's not intended as a mass-market book. It's a major piece of scholarship, and has all of the equations in the original form. It's a very rewarding read, since it tells you a lot about how the basic ideas about chance, probability and statistics were put together from the 17th century to the 19th century, but it's not for the light hearted!}  \vspace*{6pt}


%\item Andy Field (200X). Discovering statistics using SPSS (3rd ed).  \vspace*{6pt}

%{\it I have a love-hate relationship with this textbook. There's a lot of technical errors in the book (Field admits he's not a maths-y guy, and it kind of shows through to those of us who are!) and it's focused on a piece of software (SPSS) that we're trying to phase out, for reasons I'll explain later. For those reasons, I haven't set it as the textbook for this class. However, it's a bit of a pity, because there's a lot of things the book does extremely well. He writes in a really engaging way, and the book is bloody hilarious, insofar as that's possible for a stats textbook. Most importantly, he tries to explain that statistics aren't to be feared, and tries really hard to make the ideas accessible. A lot of my thinking about how to approach a subject that most people hate has been shaped by his book.}  \vspace*{6pt}


%\item William Hays (199x). Statistics (5th ed). \vspace*{6pt}

%{\it This is a textbook aimed at postgraduate students in psychology. It's actually the one that my wife learned most of her stats from, and she's a statistician now. It's a bit advanced for this course, but I used it a lot in thinking about what kinds of skills I should be setting you up for.}  \vspace*{6pt}

%\item Larry Wasserman (200x). All of statistics.  \vspace*{6pt}

%\item Robert Hogg, Joseph McKean \& Allen Craig (2005). Introduction to mathematical statistics (6th ed)  \vspace*{6pt}

%\item Mark Schervish (1995). The theory of statistics (2nd ed).  \vspace*{6pt}

%{\it Wasserman, Hogg et al \& Schervish are all very advanced texts. Psychology undergrads should never read these. Seriously. It took me about 10 years to be able to get to the point where I could read Schervish, which is the most hardcore of the three. I used these textbooks to make sure what I'm teaching you is actually right, and to make sure I understood all the maths underneath the lectures, but it's way, way beyond the scope of this class to look at that sort of thing!}  \vspace*{6pt}

%\end{itemize}

